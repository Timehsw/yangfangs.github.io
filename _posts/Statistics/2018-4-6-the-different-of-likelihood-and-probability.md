---
layout: post
title: 似然（likelihood）与概率（probability）的区别
categories: Statistics
description: 概率导论学习笔记
keywords: 学习笔记, 概率导论
---

  很多时候基础的概念没搞懂，或者知道了只知道照着例子套用，知其然而不知其所以然。对于知识的理解只能到达会运用的层面。而对于知识的创新运用是远远不足的。
  
  似然和概率在统计学中是经常见到的两个术语，有时候这两个概念是一个意思，有时候却有很大区别。这里梳理下这两个术语所代表的具体含义。
  
  
# 本文中数学符号及含义

| 符号 | 含义 |
|--------|--------|
| $O$    |     观测值   |
| $\theta$    |    随机过程中的参数    |
| $hat{\Theta}$   |   参数的估计     |
| $P (O &#124; \Theta)$   |    概率    |
| $L(\Theta &#124; O) $   |      释然  |


# wiki中关于“似然”和“概率”的解释

* 在频率推论中，似然函数（常常简称为似然）是一个在给定了数据以及模型中关于参数的函数。在非正式情况下，“似然”通常被用作“概率”的同义词。

* 在数理统计中，两个术语则有不同的意思。“概率”描述了给定模型参数后，描述结果的合理性，而不涉及任何观察到的数据。而“似然”则描述了给定了特定观测值后，描述模型参数是否合理。

# 例如

* **“概率”描述了给定模型参数后，描述结果的合理性，而不涉及任何观察到的数据。**

> 抛一枚均匀的硬币，拋20次，问15次拋得正面的可能性有多大？
> 这里的可能性就是"概率"，均匀的硬币就是给定参数$\Theta=0.5$，“拋20次15次正面”是观测值$O$。求概率$P (H=15 | \Theta=0.5) = ？$的概率。

* **“似然”描述了给定了特定观测值后，描述模型参数是否合理。**

> 拋一枚硬币，拋20次，结果15次正面向上，问其为均匀的可能性？
> 这里的可能性就是"似然"，“拋20次15次正面”为观测值$O$为已知，参数$\Theta=?$并不知道，求$L(\Theta | H=15) = P (H=15 | \Theta=0.5)$的最大化下的$\Theta$ 值。

# 离散随机变量角度看待“似然”与“概率”

  当我们在处理离散型随机变量时候，（例如，掷10硬币的结果这样的数据时候）。这时候我们就可以根据观测到的结果计算这种结果出现的概率概率，当然这有一个前提是硬币是均匀的，和掷硬币的事件都是独立的。
  这时我们想要计算的就是“概率”用$P (O | \Theta)$来表示。换个角度可以理解为，当给定了特定的参数$\Theta$时候，$P (O | \Theta)$就是我们观测到$O$观测值时候的概率。
  但是，当我们想来刻画一个实际的随机过程时候，我们常常并不知道$\Theta$参数是什么。我们只有观测值$O$，基于这个观测值我们往往想得到一个关于$\Theta$的估计值$hat{\Theta}$。当给定$\Theta$ 时候我们可以得到观测值$O$是$ P (O | \Theta)$。当然反过来，对于估计过程是在选择一个$hat{\Theta}$最大值，这个值就等价于真实观测值$O$的概率。换而言之，是在寻找一个值$hat{\Theta}$的最大化使得

  $$ L(\Theta | O) = P (O | \Theta) $$
  
  这个$L(\Theta | O)$就叫做似然函数。很明显这是一个在已知观测值$O$为条件关于未知参数 $\Theta$的似然函数。



# 从连续型随机变量角度看待“似然”与“概率”

  对于连续型随机变量与离散随机变量有一个非常重要的区别，就是人们不会去关注给定$\Theta$后观测值$O$得概率。因为，连续型随机变量存在无限多的结果（无限可分），这些结果是无法被穷尽的。我们给出某一个结果对应的概率是没有意义的（连续型随机变量产生的结果是无限的，落在任何一个“可能的结果”上的概率几乎都为0，也就是$ P (O | \Theta) = 0 ）$。
  
  当然，可以变换一种方式既给出落在结果区间范围上的概率，而非给出单个结果的概率，来解决这个问题。对于观测值$O$，可以用概率密度函数(PDF:probability density function)来表示为：$f(O|\Theta)$。因此，在连续的情况下，我们通过最大化以下函数来估计观察到的结果$O$：
  
  $$ L(\Theta | O) = f(O | \Theta) $$
  
  在这种情况下，我们不能在技术上断言我们找到最大化观察$O$的概率的参数值，因为我们最大化的是与观察结果$O$相关的PDF。

# “似然”和“概率”是站在两个角度看待问题

  对于这个函数：

  $$ P (O | \Theta) $$

  输入有两个：$O$表示某一个具体的数据；$\Theta$表示模型的参数。

* 如果$\Theta$是已知确定的，$O$是变量，这个函数叫做概率函数(probability function)，它描述对于不同的样本$O$，其出现概率是多少。

* 如果$O$是已知确定的，$\Theta$是变量，这个函数叫做似然函数(likelihood function), 它描述对于不同的模型参数，出现x这个样本点的概率是多少。



# 参考：
* [Wiki: Likelihood function](https:\en.wikipedia.org\wiki\Likelihood_function)

* [What is the difference between “likelihood” and “probability”?](https:\stats.stackexchange.com\questions\2641\what-is-the-difference-between-likelihood-and-probability)